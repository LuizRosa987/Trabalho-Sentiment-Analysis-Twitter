# -*- coding: utf-8 -*-
"""Cópia de trabpln.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hxpqdxTqmhjwEuNpZUV5gksT_5sy15S0
"""

import pandas as pd

#pegando os 200000 tweets
dfcsv = pd.read_csv('tweets politica em pt.csv')

#salvando na variavel df
df = dfcsv.to_json("politica", indent = 1, orient = 'records')

#passando a variavel para json
df = pd.read_json('politica')

"""import json"""

#excluindo tweets duplicados
df.drop_duplicates(['content'], inplace = True)

df.shape

# instalação biblioteca nltk
!pip install nltk

df.content

# importação das bibliotecas nltk
from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.corpus import stopwords
import nltk

# download dos stopwords do nltk
nltk.download('stopwords')

# pegando os stopwords em pt-br e filtrando-os do dataframe em json
stopWords = set(stopwords.words('portuguese'))
wordsFiltered = []

for w in df.content:
    if w not in stopWords:
        wordsFiltered.append(w)

print(wordsFiltered)

#importando o spaCy
!pip install spaCy

!pip install -U spacy-lookups-data

!python -m spacy download pt_core_news_lg

#importação das bibliotecas spacy numpy pandas e nltk talvez nao precisasse mas vou deixar
import pandas as pd
import spacy 
import nltk
import re
import numpy as np
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
nltk.download('punkt')
pln = spacy.load('pt_core_news_lg')

#metodo de remover acentos do professor
def remove_acentos(df):
    df = df.str.replace('[àáâãäå]', 'a')
    df = df.str.replace('[èéêë]', 'e')
    df = df.str.replace('[ìíîï]', 'i')
    df = df.str.replace('[òóôõö]', 'o')
    df = df.str.replace('[ùúûü]', 'u')
    df = df.str.replace('[ç]', 'c')
    return df

df['content'] = remove_acentos(df['content'])

df

#metodo de converter para letra minusculas tbm do professor
def conversao_letras_minusculas(df):
    df = df.str.lower()
    return df

df['content'] = conversao_letras_minusculas(df['content'])

df.content

# tirar potuação
def remove_pontuacao(df):
    df = df.str.replace('[^\w\s]', '')
    return df

df['content'] = remove_pontuacao(df['content'])

df.content

# remove url liks do df
def remove_url(df):
    df = df.str.replace('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '')
    return df

df['content'] = remove_url(df['content'])

df.content

# remove tags 
def remove_tag(df):
    df = df.str.replace(r'<[^<>]*>', '', regex=True)
    return df

df['content'] = remove_tag(df['content'])

df.content

df.info()

# aqui o dataframe em json da linha content foi passado para string pois os metodos do professor que contem lambda não conseguiam trabalhar com tipo object problema que estava dando em sala de aula
df['content'] = df['content'].astype(str)

df.info()

# lematização agrupar palavras para sua forma base pegando o radical dela
def lematizacao(df):
    df = df.apply(lambda x: ' '.join([word.lemma_ for word in pln(x)]))
    return df

df['content'] = lematizacao(df['content'])



def remove_palavras_raras(df, n_palavras):
    palavras = []
    textos = df.apply(nltk.word_tokenize)
    for texto in textos:
        for palavra in texto:
            palavras.append(palavra)
    freq = [x for x in nltk.FreqDist(palavras)]
    raras = freq[-n_palavras:]
    df = df.apply(lambda x: ' '.join([word for word in x.split() if word not in (raras)]))
    return df

df['content'] = remove_palavras_raras(df['content'], 20)

def remove_palavras_frequentes(df, n_palavras):
    palavras = []
    textos = df.apply(nltk.word_tokenize)
    for texto in textos:
        for palavra in texto:
            palavras.append(palavra)
    freq = [x for x in nltk.FreqDist(palavras)]
    frequentes = freq[0:n_palavras]
    df = df.apply(lambda x: ' '.join([word for word in x.split() if word not in (frequentes)]))
    return df

df['content'] = remove_palavras_frequentes(df['content'], 20)

def remove_numeros(df):
    df = df.apply(lambda x: ' '.join([word for word in x.split() if not word.isdigit()]))
    return df

df['content'] = remove_numeros(df['content'])

def palavras_vazias(df):
    stopwords = pln.Defaults.stop_words
    df = df.apply(lambda x: ' '.join([word for word in x.split() if word not in (stopwords)]))
    return df

df['content'] = palavras_vazias(df['content'])

df.content

dfstringfy = df.content

#salvando dataframe so que agora tratado com os metodos acima
dfstringfy.to_csv('dftratado')